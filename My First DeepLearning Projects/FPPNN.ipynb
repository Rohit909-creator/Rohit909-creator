{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FPPNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxWUJjrOZif7Q2szYLQvKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit909-creator/Rohit909-creator/blob/main/FPPNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ai1V9CjUc0R"
      },
      "source": [
        "FPPNN class for ease of use\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny73SkXKJupA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "twfY9zYx880f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hic0SEtEKC4e"
      },
      "source": [
        "'''\n",
        "\n",
        "#Neural Network Pathway\n",
        "class FPNN(nn.Module):\n",
        "\n",
        "  def __init__(self,in_channels,out_channels,hidden_size,output_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cpnn = nn.Sequential(\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size=(2,2),stride = (1,1)),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.ReLU(nn.Conv2d(out_channels,2*out_channels,kernel_size=(2,2),stride = (1,1))),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Linear(12*119*159,hidden_size,bias = True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,output_size)\n",
        "    \n",
        "\n",
        "    )\n",
        "\n",
        "    self.cppnn = nn.Sequential(\n",
        "       nn.Conv2d(in_channels,out_channels,kernel_size=(2,2),stride = (1,1)),\n",
        "       nn.LeakyReLU(),\n",
        "       nn.MaxPool2d(2),\n",
        "       nn.Flatten(1,-1),\n",
        "       nn.Linear(457446,hidden_size,bias = True),\n",
        "       nn.LeakyReLU(),\n",
        "       nn.Linear(hidden_size,hidden_size,bias = True),\n",
        "       nn.LeakyReLU(),\n",
        "       nn.Linear(hidden_size,40)\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    self.fpnn = nn.Sequential(\n",
        "        nn.Linear(10,hidden_size,bias  = True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,hidden_size,bias = True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size,4)\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "  \n",
        "  def forward(self,X,img):\n",
        "    \n",
        "    out = self.epnn(X)\n",
        "    img2 = img\n",
        "    curvature_matrix = []\n",
        "    for i in range(1,10):\n",
        "      img = img2\n",
        "      cv2.circle(img,(out[i - 1].item(),out[i].item()),5,(0,0,255),3)\n",
        "      img_data = torch.tensor([img],dtype = torch.float32)\n",
        "      out2 = self.cpnn(img_data).item()\n",
        "      curvature_matrix.append(out2)\n",
        "    curvature_tensor = torch.tensor([curvature_matrix],dtype = torch.float32)\n",
        "    output = self.fpnn(curvature_tensor)\n",
        "    return output\n",
        "\n",
        " \n",
        "  def train_epnn(self,X,Y,num_epochs,epoch_gap):\n",
        "    optimizer = torch.optim.Adam(self.epnn.parameters(),lr = 0.0001)\n",
        "    loss = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      out = self.epnn(X)\n",
        "      l = loss(out,Y)\n",
        "      optimizer.zero_grad()\n",
        "      if epoch%epoch_gap == 0:\n",
        "        print(f\"Loss{l} Epoch {epoch}\") \n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(self.epnn(X))\n",
        "\n",
        "  def train_cpnn(self,X,Y,num_epochs,epoch_gap):\n",
        "    optimizer = torch.optim.Adam(self.cpnn.parameters(),lr = 0.01)\n",
        "    loss = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      out = self.cpnn(X)\n",
        "      l = loss(out,Y)\n",
        "      optimizer.zero_grad()\n",
        "      if epoch%epoch_gap == 0:\n",
        "        print(f\"Loss{l} Epoch {epoch}\") \n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(self.epnn(X))\n",
        "\n",
        "  def train_fpnn(self,X,Y,num_epochs,epoch_gap):\n",
        "    optimizer = torch.optim.Adam(self.fpnn.parameters(),lr = 0.0001)\n",
        "    loss = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      out = self.fpnn(X)\n",
        "      l = loss(out,Y)\n",
        "      optimizer.zero_grad()\n",
        "      if epoch%epoch_gap == 0:\n",
        "        print(f\"Loss{l} Epoch {epoch}\") \n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    print(self.epnn(X))\n",
        "    \n",
        "''' \n",
        "\n",
        "class FPPNN(nn.Module):\n",
        "\n",
        "  def __init__(self,input_size,hidden_size,output_size,Attention):\n",
        "    super().__init__()\n",
        "\n",
        "    self.multi_headed_attention = Attention()\n",
        "    self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=(2,2),stride = (1,1)),\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(out_channels,2*out_channels,kernel_size=(2,2),stride = (1,1))\n",
        "    self.fc1 = nn.Linear(12*119*159,hidden_size,bias = True)\n",
        "    self.fc2 = nn.Linear(embed_size,hidden_size)\n",
        "    self.out = nn.Linear(hidden_size,output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "class Multi_Headed_Attention(nn.Module):\n",
        "  \n",
        "  def __init__(self,dim_size,embed_size):\n",
        "\n",
        "    self.queries = nn.Linear(dim_size,hidden_size)\n",
        "    self.keys = nn.Linear(dim_size,hidden_size)\n",
        "    self.values = nn.Linear(dim_size,hidden_size)\n",
        "    self.softmax = nn.Softmax(dim = 0)\n",
        "\n",
        "  def forward(self,X):\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDEI6ilKJ5CI"
      },
      "source": [
        "#image data making\n",
        "\n",
        "frames = []\n",
        "\n",
        "for i in range(2,10):\n",
        "  im = cv2.imread(f'img_data{i}.jpg')\n",
        "  frames.append(im)\n",
        "\n",
        "frames = np.array(frames,dtype = np.float32)\n",
        "#train_data = torch.tensor(frame,dtype = torch.float32)\n",
        "target_data = torch.tensor([[357,132,379,119,407,122,445,152,469,186,479,219,466,257,444,269,418,284,400,292,379,303,344,317,297,321,283,303,266,278,242,225,245,186,271,168,303,155,339,138],[377,122,396,118,417,122,432,127,449,145,465,179,481,210,503,265,507,310,482,329,458,342,423,352,384,349,362,332,345,294,330,251,312,204,318,164,331,147,352,131],[387,173,413,170,434,169,464,160,492,162,526,186,521,248,486,301,412,369,366,386,331,377,294,342,265,290,234,203,235,160,245,104,264,74,320,81,351,114,375,157],[397,148,430,181,462,219,453,258,418,309,390,332,353,364,310,382,245,390,203,364,200,313,225,274,249,251,268,217,253,168,231,103,253,66,284,71,304,82,345,105],[440,108,471,156,504,214,486,240,431,273,388,283,332,292,269,275,238,247,248,199,292,173,326,152,329,113,323,62,335,34,371,39,395,60,412,75,426,90,438,101],[439,181,473,194,508,210,504,245,500,277,494,312,486,340,457,334,438,331,412,323,381,314,349,301,316,288,277,275,245,265,251,232,268,165,281,121,308,129,342,142],[313,112,358,114,366,151,381,210,399,297,414,370,390,374,336,382,301,387,269,390,243,392,236,358,232,318,226,273,223,235,219,197,212,117,244,112,264,112,307,110],[481,108,513,139,505,166,495,194,482,235,466,274,444,332,418,322,391,307,345,278,316,258,330,226,345,190,371,136,391,99,412,66,430,77,447,88,461,97,477,107]],dtype = torch.float32)\n",
        "\n",
        "X = torch.tensor(frames,dtype=torch.float32)\n",
        "X = X.transpose(3,1)\n",
        "X = X.transpose(3,2)\n",
        "\n",
        "mean = torch.mean(X)\n",
        "var = torch.var(X)\n",
        "X_normalized = (X - mean)/torch.sqrt(var)\n",
        "X_normalized = X_normalized.to(device)\n",
        "\n",
        "Y_mean = torch.mean(target_data)\n",
        "Y_var = torch.var(target_data)\n",
        "\n",
        "Y_normalized = (target_data - Y_mean)/torch.sqrt(Y_var)\n",
        "Y_normalized = Y_normalized.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(X_normalized)\n",
        "#print(Y_normalized)\n",
        "print(X_normalized.shape)\n"
      ],
      "metadata": {
        "id": "lG9SawFUkuod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "epoch_gap = 100\n",
        "model = FPNN(3,6,256,10)\n",
        "model = model.to(device)\n",
        "\n",
        "model.train_cppnn(X_normalized,Y_normalized,num_epochs,epoch_gap)\n",
        "\n"
      ],
      "metadata": {
        "id": "n3QUXAx9fTMh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47be3e98-7be6-418b-e6be-c2ae38c9523b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss0.9842380881309509 Epoch 0\n",
            "Loss0.35492539405822754 Epoch 100\n",
            "Loss0.2554140090942383 Epoch 200\n",
            "Loss0.22441573441028595 Epoch 300\n",
            "Loss0.20562534034252167 Epoch 400\n",
            "Loss0.1899828016757965 Epoch 500\n",
            "Loss0.17419855296611786 Epoch 600\n",
            "Loss0.15741918981075287 Epoch 700\n",
            "Loss0.14013388752937317 Epoch 800\n",
            "Loss0.1236763522028923 Epoch 900\n",
            "Loss0.122135691344738 Epoch 1000\n",
            "Loss0.09840027242898941 Epoch 1100\n",
            "Loss0.08746852725744247 Epoch 1200\n",
            "Loss0.07872795313596725 Epoch 1300\n",
            "Loss0.06981072574853897 Epoch 1400\n",
            "Loss0.06140802055597305 Epoch 1500\n",
            "Loss0.05425161123275757 Epoch 1600\n",
            "Loss0.048528365790843964 Epoch 1700\n",
            "Loss0.29062020778656006 Epoch 1800\n",
            "Loss0.040805794298648834 Epoch 1900\n",
            "Loss0.038028277456760406 Epoch 2000\n",
            "Loss0.03599707409739494 Epoch 2100\n",
            "Loss0.03442727401852608 Epoch 2200\n",
            "Loss0.03315340355038643 Epoch 2300\n",
            "Loss0.032035164535045624 Epoch 2400\n",
            "Loss0.038151271641254425 Epoch 2500\n",
            "Loss0.030994325876235962 Epoch 2600\n",
            "Loss0.029369845986366272 Epoch 2700\n",
            "Loss0.02803771197795868 Epoch 2800\n",
            "Loss0.02733558975160122 Epoch 2900\n",
            "Loss0.02529146708548069 Epoch 3000\n",
            "Loss0.023605627939105034 Epoch 3100\n",
            "Loss0.022810468450188637 Epoch 3200\n",
            "Loss1.6834983825683594 Epoch 3300\n",
            "Loss0.019338607788085938 Epoch 3400\n",
            "Loss9.45367431640625 Epoch 3500\n",
            "Loss0.19392165541648865 Epoch 3600\n",
            "Loss0.11811430752277374 Epoch 3700\n",
            "Loss0.0877988263964653 Epoch 3800\n",
            "Loss0.07734304666519165 Epoch 3900\n",
            "Loss0.06736256927251816 Epoch 4000\n",
            "Loss0.06600373983383179 Epoch 4100\n",
            "Loss0.05360867455601692 Epoch 4200\n",
            "Loss0.059200920164585114 Epoch 4300\n",
            "Loss0.05384570360183716 Epoch 4400\n",
            "Loss0.048422377556562424 Epoch 4500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d9a605fb9e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_cpnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_normalized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_gap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-241912bd7513>\u001b[0m in \u001b[0;36mtrain_cpnn\u001b[0;34m(self, X, Y, num_epochs, epoch_gap)\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepoch_gap\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss{l} Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next I will have to train the fpnn"
      ],
      "metadata": {
        "id": "GbhOdBlMEUxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And also the curvature prediction NeuralNetwork"
      ],
      "metadata": {
        "id": "hEM_zesXY3Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "gRZZYu_CZksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = take_photo()\n",
        "\n",
        "img = cv2.imread(image)\n",
        "\n",
        "img_data = torch.tensor([img],dtype = torch.float32)\n",
        "\n",
        "model.cppn(img_data)"
      ],
      "metadata": {
        "id": "0sfcaPqTqnpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YkNw6C6ua9Hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOIuptNQtZcN"
      },
      "outputs": [],
      "source": [
        "# prompt: provide a base template code for processing audio data to pytorch tensor\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def audio_to_tensor(audio_path, sample_rate=16000):\n",
        "  \"\"\"\n",
        "  Converts an audio file to a PyTorch tensor.\n",
        "\n",
        "  Args:\n",
        "    audio_path (str): Path to the audio file.\n",
        "    sample_rate (int): Sample rate of the audio file.\n",
        "\n",
        "  Returns:\n",
        "    torch.Tensor: The audio data as a PyTorch tensor.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the audio file\n",
        "  audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
        "\n",
        "  # Convert the audio to a NumPy array\n",
        "  audio = np.array(audio, dtype=np.float32)\n",
        "\n",
        "  # Create a PyTorch tensor from the audio array\n",
        "  tensor = torch.from_numpy(audio)\n",
        "\n",
        "  return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "M = 3872848\n",
        "data = []\n",
        "music_files = os.listdir(\"./\")[1:]\n",
        "m = 0\n",
        "for music_file in music_files:\n",
        "  try:\n",
        "    audio_data = audio_to_tensor(music_file)\n",
        "    audio_data = F.pad(audio_data.reshape(1,-1), (0,M-audio_data.shape[0],0,0), mode=\"constant\", value=0)\n",
        "    data.append(audio_data)\n",
        "    print(audio_data.shape)\n",
        "    m = max(m, audio_data.shape[0])\n",
        "  except UserWarning as e:\n",
        "    pass\n",
        "\n",
        "  except IsADirectoryError as e:\n",
        "    pass\n",
        "print(m)\n",
        "data = torch.stack(data)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlWcPSglpbg2",
        "outputId": "4b6d31dd-8cdc-446a-c312-4457311a32d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3872848])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77ecda3be49b>:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(audio_path, sr=sample_rate)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3872848])\n",
            "torch.Size([1, 3872848])\n",
            "torch.Size([1, 3872848])\n",
            "torch.Size([1, 3872848])\n",
            "torch.Size([1, 3872848])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77ecda3be49b>:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3872848])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77ecda3be49b>:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3872848])\n",
            "torch.Size([1, 3872848])\n",
            "1\n",
            "torch.Size([9, 1, 3872848])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-77ecda3be49b>:20: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O41RBTGu71-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example tensor\n",
        "tensor = torch.tensor([[1, 2], [3, 4]])\n",
        "\n",
        "# Pad the tensor with zeros to the desired shape\n",
        "padded_tensor = F.pad(tensor, (0, 1, 0, 0), mode='constant', value=0)\n",
        "\n",
        "print(\"Original tensor:\")\n",
        "print(tensor)\n",
        "print(\"Padded tensor:\")\n",
        "print(padded_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obcrcZp95vM9",
        "outputId": "02cb0a52-a431-4811-991f-acd60a8c1c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Padded tensor:\n",
            "tensor([[1, 2, 0],\n",
            "        [3, 4, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = \"Dippam_Dappam__From__Kaathuvaakula_Rendu_Kaadhal__(256k).mp3\"\n",
        "audio_data = audio_to_tensor(audio_path)\n",
        "print(audio_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ivbdesugdV",
        "outputId": "ef05ee28-cd04-4e5d-c1da-4c35c5fe3ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.1842e-07,  1.0299e-07,  1.6614e-08,  ..., -1.0779e-06,\n",
            "         4.3498e-05, -4.0521e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now make a function to convert pytorch tensor back to wav or mp3 and save , things to consider AttributeError: No librosa attribute output\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "\n",
        "\n",
        "def tensor_to_audio(tensor, sample_rate=16000, output_path=\"audio.wav\"):\n",
        "  \"\"\"\n",
        "  Converts a PyTorch tensor to an audio file.\n",
        "\n",
        "  Args:\n",
        "    tensor (torch.Tensor): The audio data as a PyTorch tensor.\n",
        "    sample_rate (int): Sample rate of the audio file.\n",
        "    output_path (str): Path to save the audio file.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert the tensor to a NumPy array\n",
        "  audio = tensor.numpy()\n",
        "\n",
        "  # Convert the audio array to a WAV file\n",
        "  print('aga')\n",
        "  wavfile.write('Reconstructedsongforcontin'+'.wav', sample_rate, audio)\n",
        "  # wavfile.write('origal_songforcontin.wav', sample_rate, original_song)\n",
        "  # pickle.dump(enc_song, open( \"enc_song.p\", \"wb\" ) )\n",
        "\n",
        "# audio_data = torch.randn(10000)\n",
        "# tensor_to_audio(audio_data)\n"
      ],
      "metadata": {
        "id": "nCPxZV2eu38E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(M,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,12),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(12,6)  # N * 3\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(6,12),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(12,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,M),  # N * 3\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    # Note if [-1,1] use Tanh\n",
        "    return decoded\n"
      ],
      "metadata": {
        "id": "Qvim7XSYvI2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder().cuda()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 1e-2)\n"
      ],
      "metadata": {
        "id": "PClOtc5c_Y76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[0:2].reshape(2,-1).cuda()\n",
        "print(train_data.shape)"
      ],
      "metadata": {
        "id": "Osoc9hnYAEuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe56b006-22b1-4be2-ad62-db0597ce70c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3872848])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "outputs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # for (audio_data,_) in dataloader:\n",
        "  recon = model(train_data)\n",
        "  loss = criterion(recon,train_data)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.empty_cache()\n",
        "\n",
        "  print(f'Epoch:{epoch}, Loss:{loss.item():.4f}')\n",
        "  # outputs.append((epoch,audio_data,recon))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "OmaPBkFb_ZYQ",
        "outputId": "14ad3014-fdc4-4da1-d2d8-95e2e326a64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.47 GiB is free. Process 5678 has 13.28 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 6.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a7dca670ffba>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.47 GiB is free. Process 5678 has 13.28 GiB memory in use. Of the allocated memory 13.14 GiB is allocated by PyTorch, and 6.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(audio_data)"
      ],
      "metadata": {
        "id": "tQdmZfZUAwuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xe2eT7SAxXs",
        "outputId": "85703bcd-4994-41b0-df8d-92e355354a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0444, -0.0194,  0.0690,  ..., -0.0640,  0.0287,  0.0246]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIQogmmUBb5m",
        "outputId": "5c65c667-06e3-4fba-8e5c-98768041f9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0052e-05,\n",
              "         -8.0639e-05, -7.4698e-05]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.cpu().detach()"
      ],
      "metadata": {
        "id": "QcVagyUfBdpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.reshape(-1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT-F32i0C7jB",
        "outputId": "ed213a34-348c-48b8-f43e-6d832e0b3a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([242304])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.min(), output.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QrPr4sQBpHW",
        "outputId": "78eba12d-5674-4ac6-fd0d-6922db13bc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.4705) tensor(0.4394)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_to_audio(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_CiNEwgBnNW",
        "outputId": "adb7f39c-b0c3-4b03-9e66-b8e411e119e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aga\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output = output.clamp(min=0, max=32767)\n",
        "!pip install noisereduce --q"
      ],
      "metadata": {
        "id": "88jyFI5TCUfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "import noisereduce as nr\n",
        "# load data\n",
        "rate, data = wavfile.read(\"Reconstructedsongforcontin.wav\")\n",
        "# perform noise reduction\n",
        "print(rate)\n",
        "reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
        "wavfile.write(\"mywav_reduced_noise.wav\", rate, reduced_noise)"
      ],
      "metadata": {
        "id": "CMNKUpSFCdm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99342ea-35f8-43a9-f843-20983d4a23cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2B98J_MZbWWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}